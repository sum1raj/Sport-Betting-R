library(tidyverse)
options(scipen=999)
library(xgboost)
library(modelr)
set.seed(123)
library(tidymodels)
library(mlr)
library(caret)
library(parallel)
library(parallelMap) 
parallelStartSocket(cpus = detectCores())

newdata <- read_csv('newdata.csv')

## dummy variables for grouping categorical variables
dmy <- dummyVars(" ~ POS", data = newdata)
df1 <- data.frame(predict(dmy, newdata))

### join data. remove any original variables which dummies have been created for
vars <- cbind(newdata, df1) %>% select(-c(POS))
rm(list = setdiff(ls(), "vars"))#free up storage
gc()

#Define training and testing set
msplit <- initial_split(vars, strata = GAME,ID, prop = .75) 
train <- training(msplit)
test <- testing(msplit)

## hyperparameter testing
traintask <- makeRegrTask (data = train,target = "W")
testtask <- makeRegrTask (data = test,target = "W")

lrn <- makeLearner("regr.xgboost")
lrn$par.vals <- list( objective="reg:squarederror", eval_metric="rmse")

param <- makeParamSet( makeDiscreteParam("booster",values = "gbtree"),
                        makeDiscreteParam("tree_method",values = "hist"),
                        makeIntegerParam("max_depth",lower = 5L,upper = 12L), 
                        makeIntegerParam("min_child_weight",lower = 4L,upper = 10L),
                        makeNumericParam("colsample_bytree",lower = 0.2,upper = 1),
                        makeNumericParam("subsample",lower = 0.3,upper = 1), 
                        makeIntegerParam("gamma",lower = 0L,upper = 3L),
                        makeIntegerParam("alpha",lower = 0L,upper = 3L),
                        makeIntegerParam("lambda",lower = 0L,upper = 1L),
                        makeDiscreteParam("nrounds", 
                                          values = c(100, 200, 300, 400, 500, 600)),
                        makeDiscreteParam("eta",
                                          values = c(.01, .03, .05, .075, .1, .15)))


rdesc <- makeResampleDesc("CV", iters=5L)
ctrl <- makeTuneControlRandom(maxit = 25L)

# parameter tuning
mytune <- tuneParams(learner = lrn, task = traintask, resampling = rdesc,
                     par.set = param, control = ctrl, show.info = T)

##Observe rmse results
sqrt(mytune$y)

#set hyperparameters
lrn_tune <- setHyperPars(lrn,par.vals = mytune$x)

#train model
xgmodel1 <- train(learner = lrn_tune,task = traintask)

#get feature importance
imp <- mlr::getFeatureImportance(xgmodel1)
view(imp$res)

#predict on test data
xgpred <- predict(xgmodel1,testtask) %>% 
as.data.frame()
RMSE(xgpred$truth, xgpred$response)
fullmod <- makeRegrTask (data = vars,target = "W")

xgmodel2 <- train(learner = lrn_tune,task = fullmod)

saveRDS(xgmodel2, file = "MODEL1.rds")
